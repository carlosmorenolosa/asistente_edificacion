# --------------------------------------------------
# Asistente T√©cnico Inteligente para Construcci√≥n
# Versi√≥n: UI¬†Minimalista¬†Oscura (abril¬†2025)
# --------------------------------------------------
#¬†üëâ¬†Solo se modifican estilos y experiencia visual.
# --------------------------------------------------

import streamlit as st
import google.generativeai as genai
from pinecone import Pinecone
from datetime import datetime

# ---------------- Configuraci√≥n -----------------
GENAI_API_KEY = st.secrets["general"]["genai_api_key"]
PINECONE_API_KEY = st.secrets["general"]["pinecone_api_key"]
INDEX_NAME = "documentacion-edificacion"
MIN_SIMILARITY_SCORE = 0.50  # 70‚Äâ%

# ---------------- Inicializaci√≥n -----------------
genai.configure(api_key=GENAI_API_KEY)
model = genai.GenerativeModel("gemini-2.0-flash")

pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index(INDEX_NAME)

# ---------------- Estilos globales -----------------

ACCENT = "#1E88E5"           # azul acento
BG_DARK = "#0f1117"          # fondo principal
BG_ELEV = "#1a1d24"          # contenedores elevados
TEXT_LIGHT = "#e0e0e0"

st.set_page_config(
    page_title="Asistente T√©cnico Inteligente",
    page_icon="üèóÔ∏è",
    layout="wide",
)

# Inyectamos CSS minimalista oscuro
st.markdown(
    f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap');
        html, body, [class*="css"]  {{
            font-family: 'Inter', sans-serif;
            background-color: {BG_DARK};
            color: {TEXT_LIGHT};
        }}
        .block-container {{
            padding-top: 1.5rem;
            padding-bottom: 2rem;
        }}
        /* Cabecera */
        .app-header {{
            background: linear-gradient(90deg, {ACCENT} 0%, #673ab7 100%);
            padding: 1rem 2rem;
            border-radius: 0 0 12px 12px;
            margin-bottom: 1.5rem;
        }}
        .app-header h1 {{
            font-weight: 600;
            font-size: 1.6rem;
            color: #fff;
            margin: 0;
        }}
        /* Fragmentos */
        .fragment-container {{
            border-left: 3px solid {ACCENT};
            background-color: {BG_ELEV};
            padding: 0.75rem 1rem;
            border-radius: 6px;
            margin-bottom: 0.75rem;
        }}
        .fragment-source {{
            font-weight: 600;
            color: {ACCENT};
            font-size: 0.8rem;
            margin-bottom: 2px;
        }}
        .fragment-score {{
            font-size: 0.7rem;
            color: #9e9e9e;
            margin-bottom: 4px;
        }}
        .fragment-content {{
            font-size: 0.85rem;
            white-space: pre-wrap;
        }}
        /* Chat tweaks */
        .stChatMessage {{
            background-color: transparent;
        }}
        .stChatMessage .stMarkdown p {{
            margin-bottom: 0.4rem;
        }}
        /* Sidebar oscuro */
        section[data-testid="stSidebar"] > div:first-child {{
            background-color: {BG_ELEV};
        }}
    </style>
    """,
    unsafe_allow_html=True,
)

# ---------------- Prompt Base (sin cambios) -----------------
custom_prompt = """
Eres un asistente t√©cnico inteligente especializado en documentaci√≥n de ingenier√≠a civil y proyectos de construcci√≥n.
Tu objetivo es proporcionar respuestas precisas, t√©cnicas y claras basadas √∫nicamente en la documentaci√≥n proporcionada.

Recibir√°s:
1. El historial de la conversaci√≥n actual
2. La √∫ltima consulta (la actual) del usuario, la cual tienes que responder.
3. Fragmentos relevantes de documentaci√≥n t√©cnica

Instrucciones:
- Utiliza el historial de la conversaci√≥n para entender el contexto de la consulta
- Responde principalmente con informaci√≥n contenida en los fragmentos proporcionados
- Usa terminolog√≠a t√©cnica apropiada para ingenieros
- Si la documentaci√≥n no contiene la informaci√≥n solicitada, ind√≠calo claramente
- S√© conciso pero completo en tus respuestas
- Cita n√∫meros de secci√≥n, especificaciones o normas t√©cnicas cuando est√©n disponibles en los fragmentos
- Cuando identifiques la respuesta, cita textualmente de qu√© parte de la documentaci√≥n la has sacado.

Tu estilo debe ser t√©cnico, preciso y objetivo.
"""

# ----------------¬†Estado de sesi√≥n -----------------
if "conversation" not in st.session_state:
    st.session_state.conversation = []

# ---------------- Funciones auxiliares -----------------

def display_fragments(fragments):
    if not fragments:
        st.info("No se encontraron fragmentos relevantes para esta consulta.")
        return
    for frag in fragments:
        st.markdown(
            f"""
            <div class=\"fragment-container\">
                <div class=\"fragment-source\">üìÑ¬†{frag['documento']}</div>
                <div class=\"fragment-score\">Similitud¬†{frag['score']:.0%}</div>
                <div class=\"fragment-content\">{frag['texto']}</div>
            </div>
            """,
            unsafe_allow_html=True,
        )


def format_conversation_history(history):
    return "\n\n".join(f"{m['role']}: {m['content']}" for m in history)

# ---------------- Cabecera -----------------
with st.container():
    st.markdown(
        """
        <div class=\"app-header\">
            <h1>üèóÔ∏è¬†Asistente T√©cnico Inteligente</h1>
        </div>
        """,
        unsafe_allow_html=True,
    )

# ---------------- Sidebar -----------------
with st.sidebar:
    st.header("Gu√≠a r√°pida")
    st.caption("Formula tu consulta t√©cnica. La IA buscar√° en la documentaci√≥n oficial.")
    st.divider()
    st.caption(f"Versi√≥n UI¬†Oscura¬†‚Äì {datetime.utcnow():%b¬†%Y}")

# ---------------- Historial -----------------
for m in st.session_state.conversation:
    role = "assistant" if m["role"] == "Asistente" else "user"
    with st.chat_message(role):
        st.markdown(m["content"])
        if role == "assistant" and "fragments" in m:
            with st.expander("üìö¬†Fragmentos"):
                display_fragments(m["fragments"])

# ---------------- Entrada -----------------
user_message = st.chat_input("Pregunta algo sobre la normativa‚Ä¶")

if user_message:
    st.session_state.conversation.append({"role": "Usuario", "content": user_message})
    with st.chat_message("user"):
        st.markdown(user_message)

    with st.spinner("Consultando documentaci√≥n‚Ä¶"):
        embed_result = genai.embed_content(
            model="models/text-embedding-004",
            content=user_message,
        )
        query_vector = embed_result.get("embedding")

        if not query_vector:
            st.error("Error al generar el vector de embedding.")
        else:
            qres = index.query(vector=query_vector, top_k=10, include_metadata=True)
            frags = []
            for m in qres.get("matches", []):
                score = m.get("score", 0)
                if score >= MIN_SIMILARITY_SCORE:
                    md = m.get("metadata", {})
                    texto = md.get("texto", "")
                    doc = md.get("documento", "Sin nombre")
                    if texto:
                        frags.append({"texto": texto, "documento": doc, "score": score})

            ctx = "\n---\n".join(f"[{f['documento']}]: {f['texto']}" for f in frags)
            history_for_prompt = st.session_state.conversation[:-1]
            full_prompt = (
                f"{custom_prompt}\n\n"
                f"Historial:\n{format_conversation_history(history_for_prompt)}\n\n"
                f"Fragmentos:\n{ctx}\n\n"
                f"Consulta: {user_message}"
            )
            resp = model.generate_content(full_prompt)
            text = resp.candidates[0].content.parts[0].text
            st.session_state.conversation.append({"role": "Asistente", "content": text, "fragments": frags})

    with st.chat_message("assistant"):
        st.markdown(text)
        with st.expander("üìö¬†Fragmentos"):
            display_fragments(frags)
        st.toast("Respuesta lista ‚úîÔ∏è", icon="ü§ñ")

