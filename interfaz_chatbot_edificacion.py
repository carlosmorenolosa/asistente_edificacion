# --------------------------------------------------
# Asistente TÃ©cnico Inteligente para ConstrucciÃ³n
# VersiÃ³n: UIÂ Mejorada (abrilÂ 2025)
# --------------------------------------------------
#Â ğŸ‘‰Â La lÃ³gica de negocio RAG se mantiene intacta; solo se han aplicado
#Â Â Â Â mejoras sustanciales en diseÃ±o y experiencia de usuario.
# --------------------------------------------------

import streamlit as st
import google.generativeai as genai
from pinecone import Pinecone
from datetime import datetime

# ---------------- ConfiguraciÃ³n -----------------
GENAI_API_KEY = st.secrets["general"]["genai_api_key"]
PINECONE_API_KEY = st.secrets["general"]["pinecone_api_key"]
INDEX_NAME = "documentacion-edificacion"
MIN_SIMILARITY_SCORE = 0.50  # 70â€‰%

# NB:Â Misma configuraciÃ³n de recuperaciÃ³n

# ---------------- InicializaciÃ³n -----------------
genai.configure(api_key=GENAI_API_KEY)
model = genai.GenerativeModel("gemini-2.0-flash")

pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index(INDEX_NAME)

# ---------------- Estilos globales -----------------
st.set_page_config(
    page_title="Asistente TÃ©cnico Inteligente",
    page_icon="ğŸ—ï¸",
    layout="wide",
    menu_items={
        "Report a bug": "mailto:soporte@tuempresa.com",
        "About": "### Asistente TÃ©cnico\nHerramienta IA para consulta de documentaciÃ³n de edificaciÃ³n."
    },
)

PRIMARY_COLOR = "#0066cc"

st.markdown(
    f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap');
        html, body, [class*="css"]  {{
            font-family: 'Inter', sans-serif;
            scroll-behavior: smooth;
        }}
        .block-container {{
            padding-top: 1.5rem;
            padding-bottom: 2rem;
        }}
        /* Encabezado */
        .app-header {{
            background: linear-gradient(90deg, {PRIMARY_COLOR} 0%, #3e8eff 100%);
            padding: 1.2rem 2rem;
            border-radius: 0 0 12px 12px;
            color: #fff;
            margin-bottom: 1.2rem;
        }}
        .app-header h1 {{
            font-weight: 600;
            font-size: 1.75rem;
            margin: 0;
        }}
        /* Fragmentos */
        .fragment-container {{
            border-left: 4px solid {PRIMARY_COLOR};
            background-color: #f5f9ff;
            padding: 0.8rem 1rem;
            border-radius: 6px;
            margin-bottom: 0.75rem;
            box-shadow: 0 1px 3px rgba(0,0,0,0.05);
        }}
        .fragment-source {{
            font-weight: 600;
            color: {PRIMARY_COLOR};
            font-size: 0.85rem;
            margin-bottom: 2px;
        }}
        .fragment-score {{
            font-size: 0.75rem;
            color: #666;
            margin-bottom: 4px;
        }}
        .fragment-content {{
            font-size: 0.88rem;
            white-space: pre-wrap;
        }}
        /* Chat message tweaks */
        .stChatMessage .stMarkdown p {{
            margin-bottom: 0.5rem;
        }}
    </style>
    """,
    unsafe_allow_html=True,
)

# ---------------- Prompt Base (sin cambios) -----------------
custom_prompt = """
Eres un asistente tÃ©cnico inteligente especializado en documentaciÃ³n de ingenierÃ­a civil y proyectos de construcciÃ³n.
Tu objetivo es proporcionar respuestas precisas, tÃ©cnicas y claras basadas Ãºnicamente en la documentaciÃ³n proporcionada.

RecibirÃ¡s:
1. El historial de la conversaciÃ³n actual
2. La Ãºltima consulta (la actual) del usuario, la cual tienes que responder.
3. Fragmentos relevantes de documentaciÃ³n tÃ©cnica

Instrucciones:
- Utiliza el historial de la conversaciÃ³n para entender el contexto de la consulta
- Responde principalmente con informaciÃ³n contenida en los fragmentos proporcionados
- Usa terminologÃ­a tÃ©cnica apropiada para ingenieros
- Si la documentaciÃ³n no contiene la informaciÃ³n solicitada, indÃ­calo claramente
- SÃ© conciso pero completo en tus respuestas
- Cita nÃºmeros de secciÃ³n, especificaciones o normas tÃ©cnicas cuando estÃ©n disponibles en los fragmentos
- Cuando identifiques la respuesta, cita textualmente de quÃ© parte de la documentaciÃ³n la has sacado.

Tu estilo debe ser tÃ©cnico, preciso y objetivo.
"""

# ----------------Â Estado de sesiÃ³n -----------------
if "conversation" not in st.session_state:
    st.session_state.conversation = []

# ---------------- Funciones auxiliares -----------------

def display_fragments(fragments):
    """Renderiza los pasajes recuperados en un contenedor elegante."""
    if not fragments:
        st.info("No se encontraron fragmentos relevantes para esta consulta.")
        return
    for frag in fragments:
        badge_color = "#28a745" if frag["score"] >= 0.8 else ("#ffc107" if frag["score"] >= 0.6 else "#dc3545")
        st.markdown(
            f"""
            <div class="fragment-container">
                <div class="fragment-source">ğŸ“„ {frag['documento']}</div>
                <div class="fragment-score"><span style='background:{badge_color};color:#fff;padding:2px 6px;border-radius:4px'>Similitud {frag['score']:.0%}</span></div>
                <div class="fragment-content">{frag['texto']}</div>
            </div>
            """,
            unsafe_allow_html=True,
        )


def format_conversation_history(history):
    return "\n\n".join(f"{m['role']}: {m['content']}" for m in history)

# ---------------- Encabezado custom -----------------
with st.container():
    st.markdown(
        """
        <div class="app-header">
            <h1>ğŸ—ï¸Â Asistente TÃ©cnico Inteligente para ConstrucciÃ³n</h1>
        </div>
        """,
        unsafe_allow_html=True,
    )

# ---------------- Sidebar -----------------
with st.sidebar:
    st.header("â„¹ï¸Â GuÃ­a rÃ¡pida")
    st.markdown(
        """
        1. Formula tu **consulta tÃ©cnica** en el cuadro inferior.
        2. Revisa la respuesta (fuente citada).
        3. Expande *ğŸ“šÂ Fragmentos recuperados* para ver el contexto.
        """
    )
    st.divider()
    st.caption("VersiÃ³n UIÂ Mejorada â€“ {:%dÂ %bÂ %Y}.".format(datetime.utcnow()))
    st.write("Â©â€¯2025Â Tu Empresa")

# ---------------- Mostrar historial -----------------
for msg in st.session_state.conversation:
    role = "assistant" if msg["role"] == "Asistente" else "user"
    with st.chat_message(role):
        st.markdown(msg["content"])
        if role == "assistant" and "fragments" in msg:
            with st.expander("ğŸ“šÂ Mostrar fragmentos recuperados"):
                display_fragments(msg["fragments"])

# ---------------- Entrada del usuario -----------------
user_message = st.chat_input("Escribe tu consulta tÃ©cnica aquÃ­â€¦")

if user_message:
    st.session_state.conversation.append({"role": "Usuario", "content": user_message})
    with st.chat_message("user"):
        st.markdown(user_message)

    with st.spinner("ğŸ”Â Analizando documentaciÃ³nâ€¦"):
        # (lÃ³gica de embedding / bÃºsqueda SIN CAMBIOS)
        embed_result = genai.embed_content(
            model="models/text-embedding-004",
            content=user_message,
        )
        query_vector = embed_result.get("embedding")

        if not query_vector:
            st.error("Error al generar el vector de embedding de la consulta.")
        else:
            query_response = index.query(vector=query_vector, top_k=10, include_metadata=True)
            retrieved_segments = []
            for match in query_response.get("matches", []):
                score = match.get("score", 0)
                if score >= MIN_SIMILARITY_SCORE:
                    md = match.get("metadata", {})
                    texto = md.get("texto", "")
                    doc = md.get("documento", "Documento sin nombre")
                    if texto:
                        retrieved_segments.append({
                            "texto": texto,
                            "documento": doc,
                            "score": score,
                        })

            retrieved_context = "\n---\n".join([
                f"[{seg['documento']}]: {seg['texto']}" for seg in retrieved_segments
            ])
            history_for_prompt = st.session_state.conversation[:-1] if len(st.session_state.conversation) > 1 else []
            formatted_history = format_conversation_history(history_for_prompt)

            full_prompt = (
                f"{custom_prompt}\n\n"
                f"ğŸ“œÂ **Historial de la conversaciÃ³n:**\n{formatted_history}\n"
                f"ğŸ“šÂ **Fragmentos de documentaciÃ³n relevantes:**\n{retrieved_context}\n\n"
                f"ğŸ‘¤Â **Consulta actual del usuario:** {user_message}"
            )

            response = model.generate_content(full_prompt)
            response_text = response.candidates[0].content.parts[0].text

            # Guardar respuesta y fragmentos
            st.session_state.conversation.append(
                {
                    "role": "Asistente",
                    "content": response_text,
                    "fragments": retrieved_segments,
                }
            )

    # Mostrar respuesta
    with st.chat_message("assistant"):
        st.markdown(response_text)
        with st.expander("ğŸ“šÂ Mostrar fragmentos recuperados"):
            display_fragments(retrieved_segments)
        st.toast("âœ…Â Respuesta generada", icon="ğŸ¤–")
